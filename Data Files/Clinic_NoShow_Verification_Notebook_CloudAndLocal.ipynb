{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79936f1a",
   "metadata": {},
   "source": [
    "\n",
    "# Clinic No-Show — Verification Notebook (Local **and** BigQuery)\n",
    "Use this notebook to recompute the findings locally **or** reproduce them in **BigQuery**.\n",
    "\n",
    "**You need** the 4 CSVs (or the all-in-one ZIP):\n",
    "- `appointments_clean.csv`\n",
    "- `weather.csv`\n",
    "- `twitter_agg.csv`\n",
    "- `zip_city.csv` (QA only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c900e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base paths (works in Colab or local Jupyter)\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "DATA_DIR = Path(\"/content/data\")  # Change to your folder if running locally\n",
    "OUTPUT_DIR = DATA_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:0.1f}\")\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc7e4f",
   "metadata": {},
   "source": [
    "## Option A (Colab): Upload ZIP or individual CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1834ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in Colab, uncomment and run this cell to upload ZIP or CSVs.\n",
    "try:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    import zipfile, io\n",
    "    for name, content in uploaded.items():\n",
    "        if name.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(io.BytesIO(content)) as zf:\n",
    "                zf.extractall(DATA_DIR)\n",
    "        else:\n",
    "            with open(DATA_DIR / name, \"wb\") as f:\n",
    "                f.write(content)\n",
    "    print(\"Uploaded to:\", DATA_DIR)\n",
    "except Exception:\n",
    "    print(\"Upload helper not available (likely not in Colab). Use Option B (local path).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b171ee",
   "metadata": {},
   "source": [
    "\n",
    "## Option B (local): Point to your folder\n",
    "If running locally, set `DATA_DIR` above to your CSV folder (e.g., `C:/Users/you/Downloads/clinic_project`) and re-run the next cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e36c7d",
   "metadata": {},
   "source": [
    "## Local path — recompute findings from CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ba762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "required = [\"appointments_clean.csv\",\"weather.csv\",\"twitter_agg.csv\"]\n",
    "missing = [fn for fn in required if not (DATA_DIR/fn).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing files in {DATA_DIR}: {missing}\")\n",
    "\n",
    "appts = pd.read_csv(DATA_DIR/\"appointments_clean.csv\")\n",
    "wx    = pd.read_csv(DATA_DIR/\"weather.csv\")\n",
    "tw    = pd.read_csv(DATA_DIR/\"twitter_agg.csv\")\n",
    "\n",
    "# Feature engineering & joins (mirror of BigQuery logic)\n",
    "appts[\"dt\"] = pd.to_datetime(appts[\"date\"])\n",
    "appts[\"booked_dt\"] = pd.to_datetime(appts[\"booked_date\"])\n",
    "appts[\"city_up\"] = appts[\"city\"].str.upper()\n",
    "appts[\"no_show\"] = (appts[\"status\"].str.lower()==\"no_show\").astype(int)\n",
    "appts[\"hour\"] = appts[\"time\"].str.slice(0,2).astype(int)\n",
    "appts[\"hour_bucket\"] = appts[\"hour\"].apply(lambda h: \"AM\" if 7<=h<=11 else (\"PM\" if 12<=h<=16 else \"Late\"))\n",
    "appts[\"lead_time_days\"] = (appts[\"dt\"] - appts[\"booked_dt\"]).dt.days.clip(lower=0)\n",
    "\n",
    "wx[\"dt\"] = pd.to_datetime(wx[\"dt\"]); wx[\"city_up\"]=wx[\"city\"].str.upper()\n",
    "tw[\"dt\"] = pd.to_datetime(tw[\"dt\"]); tw[\"city_up\"]=tw[\"city\"].str.upper()\n",
    "\n",
    "fact = appts.merge(wx[[\"dt\",\"city_up\",\"tmax_c\",\"tmin_c\",\"prcp_mm\"]], on=[\"dt\",\"city_up\"], how=\"left\") \\            .merge(tw[[\"dt\",\"city_up\",\"tweet_count_symptoms\",\"sentiment_avg\"]], on=[\"dt\",\"city_up\"], how=\"left\")\n",
    "\n",
    "fact[\"temp_bucket\"] = fact[\"tmax_c\"].apply(lambda t: \"Hot\" if t>=30 else (\"Cold\" if t<=5 else \"Mild\"))\n",
    "pth = fact[\"tweet_count_symptoms\"].quantile(0.75)\n",
    "fact[\"chatter_bucket\"] = np.where(fact[\"tweet_count_symptoms\"]>=pth, \"High\", \"Normal\")\n",
    "\n",
    "print(\"Overall no-show %:\", round(fact[\"no_show\"].mean()*100,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcccbd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lead time\n",
    "lead = (fact.assign(lead_bucket=fact[\"lead_time_days\"].apply(lambda x: \"0-2\" if x<=2 else (\"3-7\" if x<=7 else \"8+\")))\n",
    "            .groupby(\"lead_bucket\")[\"no_show\"].mean().mul(100).round(1).reindex([\"0-2\",\"3-7\",\"8+\"]).reset_index())\n",
    "display(lead); lead.to_csv(OUTPUT_DIR/\"leadtime_rates_recomputed.csv\", index=False)\n",
    "\n",
    "# Weather × time\n",
    "wx_time = fact.groupby([\"temp_bucket\",\"hour_bucket\"])[\"no_show\"].mean().mul(100).round(1).reset_index()\n",
    "display(wx_time.sort_values([\"temp_bucket\",\"hour_bucket\"])); wx_time.to_csv(OUTPUT_DIR/\"weather_time_matrix_recomputed.csv\", index=False)\n",
    "\n",
    "# Chatter\n",
    "chatter = fact.groupby(\"chatter_bucket\")[\"no_show\"].mean().mul(100).round(1).reindex([\"Normal\",\"High\"]).reset_index()\n",
    "display(chatter); chatter.to_csv(OUTPUT_DIR/\"chatter_effect_recomputed.csv\", index=False)\n",
    "\n",
    "# Top segments\n",
    "top = fact.groupby([\"clinic\",\"visit_type\",\"hour_bucket\",\"temp_bucket\"])[\"no_show\"].agg(['mean','count']).reset_index()\n",
    "top[\"no_show_rate_pct\"] = (top[\"mean\"]*100).round(1)\n",
    "top = top[top[\"count\"]>=20].sort_values(\"no_show_rate_pct\", ascending=False).head(15)\n",
    "display(top[[\"clinic\",\"visit_type\",\"hour_bucket\",\"temp_bucket\",\"no_show_rate_pct\",\"count\"]])\n",
    "top[[\"clinic\",\"visit_type\",\"hour_bucket\",\"temp_bucket\",\"no_show_rate_pct\",\"count\"]].to_csv(OUTPUT_DIR/\"top_segments_recomputed.csv\", index=False)\n",
    "\n",
    "# Overall (write file)\n",
    "overall = round(fact[\"no_show\"].mean()*100,1)\n",
    "with open(OUTPUT_DIR/\"overall_no_show.txt\",\"w\") as f: f.write(str(overall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822a24a",
   "metadata": {},
   "source": [
    "### Verify against slide numbers (±0.5 ppt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ecd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expected = {\n",
    "    \"lead\": {\"0-2\": 19.2, \"3-7\": 15.7, \"8+\": 17.2},\n",
    "    \"chatter\": {\"Normal\": 16.6, \"High\": 18.6},\n",
    "    \"overall\": 17.1,\n",
    "    \"wx_time_subset\": {(\"Hot\",\"AM\"): 17.3, (\"Hot\",\"PM\"): 18.7, (\"Mild\",\"AM\"): 16.1, (\"Mild\",\"PM\"): 17.3},\n",
    "}\n",
    "tol = 0.5\n",
    "def close(a,b,t=tol): return abs(float(a)-float(b)) <= t\n",
    "\n",
    "lead_check = {row[\"lead_bucket\"]: float(row[\"no_show\"]) for _, row in lead.iterrows()}\n",
    "chat_check = {row[\"chatter_bucket\"]: float(row[\"no_show\"]) for _, row in chatter.iterrows()}\n",
    "wx_dict = {(r[\"temp_bucket\"], r[\"hour_bucket\"]): float(r[\"no_show\"]) for _,r in wx_time.iterrows()}\n",
    "\n",
    "print(\"Lead-time PASS:\", all(k in lead_check and close(lead_check[k], v) for k,v in expected[\"lead\"].items()), lead_check)\n",
    "print(\"Chatter   PASS:\", all(k in chat_check and close(chat_check[k], v) for k,v in expected[\"chatter\"].items()), chat_check)\n",
    "print(\"Overall   PASS:\", close(overall, expected[\"overall\"]), overall)\n",
    "print(\"Wx×Time   PASS:\", all(k in wx_dict and close(wx_dict[k], v) for k,v in expected[\"wx_time_subset\"].items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d084541",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# BigQuery (cloud) path — end-to-end\n",
    "This section loads the same CSVs into **BigQuery**, creates the standardized tables and modeling view, and runs the confirmation queries.\n",
    "\n",
    "**What you need:**\n",
    "- Google account with access to a GCP project\n",
    "- Project ID (e.g., `my-gcp-project`)\n",
    "- The four CSVs already in this notebook's `DATA_DIR` (use the upload cell above if needed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install libraries (Colab-friendly). If running locally, you may already have these.\n",
    "!pip -q install google-cloud-bigquery pandas-gbq pyarrow >/dev/null\n",
    "\n",
    "# Authenticate (Colab) and set the project\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    print(\"Authenticated.\")\n",
    "except Exception as e:\n",
    "    print(\"Colab auth helper not available. If local, ensure application-default credentials are set.\")\n",
    "\n",
    "import os\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\") or input(\"Enter your GCP PROJECT_ID: \").strip()\n",
    "print(\"Using PROJECT_ID:\", PROJECT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a13aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "dataset_id = f\"{PROJECT_ID}.clinic_ops\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"US\"\n",
    "client.create_dataset(dataset, exists_ok=True)\n",
    "print(\"Dataset ready:\", dataset_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36744289",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def load_csv_to_bq(table_name, csv_path):\n",
    "    table_id = f\"{dataset_id}.{table_name}\"\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,\n",
    "        autodetect=True,\n",
    "        field_delimiter=\",\",\n",
    "        encoding=\"UTF-8\",\n",
    "    )\n",
    "    with open(csv_path, \"rb\") as f:\n",
    "        job = client.load_table_from_file(f, table_id, job_config=job_config)\n",
    "    job.result()\n",
    "    table = client.get_table(table_id)\n",
    "    print(f\"Loaded {table_id}: {table.num_rows} rows\")\n",
    "\n",
    "# Ensure files exist\n",
    "for fn in [\"appointments_clean.csv\",\"weather.csv\",\"twitter_agg.csv\",\"zip_city.csv\"]:\n",
    "    if not (DATA_DIR/fn).exists():\n",
    "        raise FileNotFoundError(f\"Missing {fn} in {DATA_DIR}. Upload or fix DATA_DIR.\")\n",
    "\n",
    "load_csv_to_bq(\"appointments_staging\", str(DATA_DIR/\"appointments_clean.csv\"))\n",
    "load_csv_to_bq(\"weather_staging\", str(DATA_DIR/\"weather.csv\"))\n",
    "load_csv_to_bq(\"twitter_staging\", str(DATA_DIR/\"twitter_agg.csv\"))\n",
    "load_csv_to_bq(\"zip_city_staging\", str(DATA_DIR/\"zip_city.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e805c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ddl = '''\n",
    "CREATE OR REPLACE TABLE `clinic_ops.appointments` AS\n",
    "SELECT\n",
    "  appt_id,\n",
    "  DATE(date) AS dt,\n",
    "  TIME(time) AS appt_time,\n",
    "  UPPER(city) AS city,\n",
    "  clinic,\n",
    "  zip,\n",
    "  visit_type,\n",
    "  new_or_return,\n",
    "  DATE(booked_date) AS booked_dt,\n",
    "  status\n",
    "FROM `clinic_ops.appointments_staging`;\n",
    "\n",
    "CREATE OR REPLACE TABLE `clinic_ops.weather` AS\n",
    "SELECT DATE(dt) AS dt, UPPER(city) AS city, tmax_c, tmin_c, prcp_mm\n",
    "FROM `clinic_ops.weather_staging`;\n",
    "\n",
    "CREATE OR REPLACE TABLE `clinic_ops.twitter` AS\n",
    "SELECT DATE(dt) AS dt, UPPER(city) AS city, tweet_count_symptoms, sentiment_avg\n",
    "FROM `clinic_ops.twitter_staging`;\n",
    "\n",
    "CREATE OR REPLACE TABLE `clinic_ops.zip_city` AS\n",
    "SELECT zip, UPPER(city) AS city\n",
    "FROM `clinic_ops.zip_city_staging`;\n",
    "\n",
    "CREATE OR REPLACE VIEW `clinic_ops.fact_day` AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    a.dt, a.city, a.clinic, a.visit_type,\n",
    "    IF(LOWER(a.new_or_return)='new', 1, 0) AS is_new_patient,\n",
    "    IF(LOWER(a.status)='no_show', 1, 0) AS no_show,\n",
    "    TIMESTAMP_DIFF(TIMESTAMP(a.dt, a.appt_time), TIMESTAMP(a.booked_dt, TIME '00:00:00'), DAY) AS lead_time_days,\n",
    "    CASE\n",
    "      WHEN EXTRACT(HOUR FROM a.appt_time) BETWEEN 7 AND 11 THEN 'AM'\n",
    "      WHEN EXTRACT(HOUR FROM a.appt_time) BETWEEN 12 AND 16 THEN 'PM'\n",
    "      ELSE 'Late'\n",
    "    END AS hour_bucket\n",
    "  FROM `clinic_ops.appointments` a\n",
    ")\n",
    "SELECT\n",
    "  b.*,\n",
    "  w.tmax_c, w.tmin_c, w.prcp_mm,\n",
    "  CASE WHEN w.prcp_mm > 0 THEN 1 ELSE 0 END AS rain_flag,\n",
    "  CASE WHEN w.tmax_c >= 30 THEN 'Hot' WHEN w.tmax_c <= 5 THEN 'Cold' ELSE 'Mild' END AS temp_bucket,\n",
    "  t.tweet_count_symptoms, t.sentiment_avg\n",
    "FROM base b\n",
    "LEFT JOIN `clinic_ops.weather` w USING (dt, city)\n",
    "LEFT JOIN `clinic_ops.twitter` t USING (dt, city);\n",
    "'''\n",
    "job = client.query(ddl)\n",
    "job.result()\n",
    "print(\"Standardized tables and view created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa9b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Lead time\n",
    "q1 = '''\n",
    "SELECT\n",
    "  CASE WHEN lead_time_days <= 2 THEN '0-2'\n",
    "       WHEN lead_time_days <= 7 THEN '3-7'\n",
    "       ELSE '8+' END AS lead_bucket,\n",
    "  ROUND(AVG(no_show)*100,1) AS no_show_rate_pct,\n",
    "  COUNT(*) AS n\n",
    "FROM `clinic_ops.fact_day`\n",
    "GROUP BY lead_bucket\n",
    "ORDER BY lead_bucket\n",
    "'''\n",
    "lead_bq = client.query(q1).to_dataframe()\n",
    "display(lead_bq)\n",
    "\n",
    "# Weather × time\n",
    "q2 = '''\n",
    "SELECT temp_bucket, hour_bucket,\n",
    "       ROUND(AVG(no_show)*100,1) AS no_show_rate_pct,\n",
    "       COUNT(*) AS n\n",
    "FROM `clinic_ops.fact_day`\n",
    "GROUP BY temp_bucket, hour_bucket\n",
    "ORDER BY temp_bucket, hour_bucket\n",
    "'''\n",
    "wx_bq = client.query(q2).to_dataframe()\n",
    "display(wx_bq)\n",
    "\n",
    "# Chatter\n",
    "q3 = '''\n",
    "WITH with_bucket AS (\n",
    "  SELECT *, CASE\n",
    "    WHEN tweet_count_symptoms >= PERCENTILE_CONT(tweet_count_symptoms, 0.75) OVER() THEN 'High'\n",
    "    ELSE 'Normal' END AS chatter_bucket\n",
    "  FROM `clinic_ops.fact_day`\n",
    ")\n",
    "SELECT chatter_bucket,\n",
    "       ROUND(AVG(no_show)*100,1) AS no_show_rate_pct,\n",
    "       COUNT(*) AS n\n",
    "FROM with_bucket\n",
    "GROUP BY chatter_bucket\n",
    "ORDER BY chatter_bucket\n",
    "'''\n",
    "chat_bq = client.query(q3).to_dataframe()\n",
    "display(chat_bq)\n",
    "\n",
    "# Top segments\n",
    "q4 = '''\n",
    "SELECT clinic, visit_type, hour_bucket, temp_bucket,\n",
    "       ROUND(AVG(no_show)*100,1) AS no_show_rate_pct,\n",
    "       COUNT(*) AS n\n",
    "FROM `clinic_ops.fact_day`\n",
    "GROUP BY clinic, visit_type, hour_bucket, temp_bucket\n",
    "HAVING COUNT(*) >= 20\n",
    "ORDER BY no_show_rate_pct DESC\n",
    "LIMIT 15\n",
    "'''\n",
    "top_bq = client.query(q4).to_dataframe()\n",
    "display(top_bq)\n",
    "\n",
    "# Overall\n",
    "q5 = 'SELECT ROUND(AVG(no_show)*100,1) AS overall_no_show_pct FROM `clinic_ops.fact_day`'\n",
    "overall_bq = client.query(q5).to_dataframe()\n",
    "display(overall_bq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save BigQuery outputs to the same OUTPUT_DIR for easy comparison\n",
    "lead_bq.to_csv(OUTPUT_DIR/\"leadtime_rates_bq.csv\", index=False)\n",
    "wx_bq.to_csv(OUTPUT_DIR/\"weather_time_matrix_bq.csv\", index=False)\n",
    "chat_bq.to_csv(OUTPUT_DIR/\"chatter_effect_bq.csv\", index=False)\n",
    "top_bq.to_csv(OUTPUT_DIR/\"top_segments_bq.csv\", index=False)\n",
    "overall_bq.to_csv(OUTPUT_DIR/\"overall_no_show_bq.csv\", index=False)\n",
    "print(\"Saved BigQuery outputs under:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf1e9c",
   "metadata": {},
   "source": [
    "\n",
    "### Optional: Load from **GCS** instead of local files\n",
    "If your files are already in a GCS bucket, you can load via `gs://bucket/path.csv` URIs. Example:\n",
    "\n",
    "```python\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    autodetect=True,\n",
    ")\n",
    "uri = \"gs://YOUR_BUCKET/clinic_project/appointments_clean.csv\"\n",
    "table_id = f\"{PROJECT_ID}.clinic_ops.appointments_staging\"\n",
    "job = client.load_table_from_uri(uri, table_id, job_config=job_config)\n",
    "job.result()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d34d5",
   "metadata": {},
   "source": [
    "## (Colab) Download all outputs as a ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850385f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import shutil\n",
    "    zip_path = \"/content/clinic_verification_outputs.zip\"\n",
    "    shutil.make_archive(\"/content/clinic_verification_outputs\", 'zip', OUTPUT_DIR)\n",
    "    from google.colab import files\n",
    "    files.download(zip_path)\n",
    "except Exception:\n",
    "    print(\"Zip/download skipped (not in Colab). Outputs are in:\", OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
